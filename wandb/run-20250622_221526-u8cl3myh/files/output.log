C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.16k/1.16k [00:00<?, ?B/s]
C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\huggingface_cache\hub\models--Qwen--Qwen-VL-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
configuration_qwen.py: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.09k/2.09k [00:00<?, ?B/s]
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-VL-Chat:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
modeling_qwen.py: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.7k/44.7k [00:00<00:00, 9.77MB/s]
visual.py: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.6k/14.6k [00:00<00:00, 27.8MB/s]
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-VL-Chat:
- visual.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
qwen_generation_utils.py: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.9k/14.9k [00:00<?, ?B/s]
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-VL-Chat:
- qwen_generation_utils.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-VL-Chat:
- modeling_qwen.py
- visual.py
- qwen_generation_utils.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
pytorch_model.bin.index.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79.9k/79.9k [00:00<00:00, 3.07MB/s]
Downloading shards:   0%|                                                                                                                                                                               | 0/10 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00001-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.96G/1.96G [04:43<00:00, 6.92MB/s]
Downloading shards:  10%|████████████████▌                                                                                                                                                     | 1/10 [04:44<42:36, 284.06s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00002-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.93G/1.93G [04:40<00:00, 6.89MB/s]
Downloading shards:  20%|█████████████████████████████████▏                                                                                                                                    | 2/10 [09:24<37:37, 282.16s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00003-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.93G/1.93G [04:42<00:00, 6.84MB/s]
Downloading shards:  30%|█████████████████████████████████████████████████▊                                                                                                                    | 3/10 [14:08<32:58, 282.67s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00004-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.99G/1.99G [05:22<00:00, 6.16MB/s]
Downloading shards:  40%|██████████████████████████████████████████████████████████████████▍                                                                                                   | 4/10 [19:31<29:52, 298.69s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00005-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.92G/1.92G [03:22<00:00, 9.50MB/s]
Downloading shards:  50%|███████████████████████████████████████████████████████████████████████████████████                                                                                   | 5/10 [22:54<22:00, 264.09s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00006-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.93G/1.93G [02:46<00:00, 11.6MB/s]
Downloading shards:  60%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                  | 6/10 [25:41<15:24, 231.14s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00007-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.93G/1.93G [02:57<00:00, 10.9MB/s]
Downloading shards:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                 | 7/10 [28:39<10:41, 213.80s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00008-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.98G/1.98G [05:21<00:00, 6.15MB/s]
Downloading shards:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 8/10 [34:01<08:16, 248.11s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00009-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.99G/1.99G [06:19<00:00, 5.25MB/s]
Downloading shards:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 9/10 [40:21<04:49, 289.35s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00010-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.73G/1.73G [05:56<00:00, 4.86MB/s]
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [46:17<00:00, 277.77s/it]
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
bin C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\bitsandbytes\libbitsandbytes_cuda121.dll
Loading checkpoint shards:   0%|                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
Loading checkpoint shards:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                | 9/10 [00:21<00:02,  2.42s/it]
Traceback (most recent call last):
  File "C:\Users\bilal\Documents\VLM_Medical\train.py", line 68, in <module>
    model = AutoModelForCausalLM.from_pretrained(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 561, in from_pretrained
    return model_class.from_pretrained(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\modeling_utils.py", line 3706, in from_pretrained
    ) = cls._load_pretrained_model(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\modeling_utils.py", line 4116, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\modeling_utils.py", line 786, in _load_state_dict_into_meta_model
    set_module_quantized_tensor_to_device(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\integrations\bitsandbytes.py", line 108, in set_module_quantized_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 8.00 GiB of which 1.03 GiB is free. Of the allocated memory 5.75 GiB is allocated by PyTorch, and 99.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
