C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.37k/1.37k [00:00<?, ?B/s]
C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\bilal\.cache\huggingface\hub\models--Qwen--Qwen2.5-VL-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Traceback (most recent call last):
  File "C:\Users\bilal\Documents\VLM_Medical\train.py", line 68, in <module>
    model = AutoModelForCausalLM.from_pretrained(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\models\auto\configuration_auto.py", line 1098, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\models\auto\configuration_auto.py", line 795, in __getitem__
    raise KeyError(key)
KeyError: 'qwen2_5_vl'
