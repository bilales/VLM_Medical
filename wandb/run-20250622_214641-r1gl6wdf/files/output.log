C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
pytorch_model.bin.index.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79.9k/79.9k [00:00<00:00, 994kB/s]
C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\bilal\.cache\huggingface\hub\models--Qwen--Qwen-VL-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Downloading shards:   0%|                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00001-of-00010.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.96G/1.96G [03:29<00:00, 9.39MB/s]
Downloading shards:  10%|████████████████▊                                                                                                                                                       | 1/10 [03:29<31:26, 209.64s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00002-of-00010.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.93G/1.93G [04:11<00:00, 7.67MB/s]
Downloading shards:  20%|█████████████████████████████████▌                                                                                                                                      | 2/10 [07:41<31:17, 234.72s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00003-of-00010.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.93G/1.93G [04:30<00:00, 7.15MB/s]
Downloading shards:  30%|██████████████████████████████████████████████████▍                                                                                                                     | 3/10 [12:12<29:19, 251.30s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
pytorch_model-00004-of-00010.bin:  33%|████████████████████████████████████████████████▎                                                                                                   | 650M/1.99G [01:41<03:29, 6.39MB/s]
Downloading shards:  30%|██████████████████████████████████████████████████▍                                                                                                                     | 3/10 [13:55<32:28, 278.35s/it]
Traceback (most recent call last):
  File "C:\Users\bilal\Documents\VLM_Medical\train.py", line 68, in <module>
    model = AutoModelForCausalLM.from_pretrained(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 561, in from_pretrained
    return model_class.from_pretrained(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\modeling_utils.py", line 3351, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\utils\hub.py", line 1017, in get_checkpoint_shard_files
    cached_filename = cached_file(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\transformers\utils\hub.py", line 389, in cached_file
    resolved_file = hf_hub_download(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py", line 1161, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py", line 1725, in _download_to_tmp_and_move
    http_get(
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\huggingface_hub\file_download.py", line 494, in http_get
    for chunk in r.iter_content(chunk_size=constants.DOWNLOAD_CHUNK_SIZE):
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\requests\models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\urllib3\response.py", line 1091, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\urllib3\response.py", line 980, in read
    data = self._raw_read(amt)
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\urllib3\response.py", line 904, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
  File "C:\Users\bilal\Documents\VLM_Medical\.venv\lib\site-packages\urllib3\response.py", line 887, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
  File "C:\Users\bilal\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 464, in read
    s = self.fp.read(amt)
  File "C:\Users\bilal\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\bilal\AppData\Local\Programs\Python\Python310\lib\ssl.py", line 1273, in recv_into
    return self.read(nbytes, buffer)
  File "C:\Users\bilal\AppData\Local\Programs\Python\Python310\lib\ssl.py", line 1129, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
